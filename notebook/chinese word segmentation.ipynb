{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文分词模型，提供目前比较常见的分词模型，结合代码进行实现。主要有词典(也叫机械分词)、N_gram、隐马尔可夫HMM、条件随机场CRF几种模型的分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ● 数据集来自SIGHAN，SIGHAN 是国际计算语言协会ACL中文处理小组的简称。目前SIGHAN bakeoff 已经举办了 6 届，其中语料资源免费。选用 icwb-data2 数据作为数据集。\n",
    "  \n",
    "  ● icwb-data2 中包含train、test、scripts、gold、doc 目录\n",
    "  \n",
    "      ○ doc：数据集的一些使用指南\n",
    "      ○ training： 包含已经分词的训练数据集目录。这里选择 msr_training.utf8 作为训练集。其他信息可见doc目录下的说明\n",
    "          ■ 文件后缀名为 utf8 的表示编码格式为 UTF-8。\n",
    "          ■ 文件前缀 msr_ ，代表是微软亚洲研究院提供。\n",
    "      ○ testing：未切分的测试数据集\n",
    "      ○ scripts：评分脚本和简单的分词器\n",
    "      ○ gold：测试数据集的标准分词和训练集中抽取的词表\n",
    "      \n",
    "  ● 数据集下载：http://sighan.cs.uchicago.edu/bakeoff2005/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词典分词\n",
    "\n",
    "关于分词的介绍参考个人云笔记：http://note.youdao.com/noteshare?id=81e18f06e7c39d59da74323cc5aff346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词典分词模型已封装在 module/segmentation/dict/dict_segmentation.py 类中。详细代码在可在此文件中查看。模型提供以下函数\n",
    "\n",
    " 1. 拟合：根据提供的训练数据创建词典。 并进行保存\n",
    " 2. 评估：读取测试数据，并对数据进行分词，写入文本\n",
    " 3. 分词：输入某个句子文本，对其进行分词\n",
    " 4. 添加新词：对与某个句子中的词汇没有正确识别，可通过添加新词方式实现。\n",
    " 5. 加载模型：对拟合保存的模型进行加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载库\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from module.segmentation.dict.dict_segmentation import DictSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "    def __init__(self, max_matching=MAX_MATCHING):\n",
    "        \"\"\"\n",
    "        初始化对象\n",
    "        :param max_matching: <int> 词的最大匹配长度。用于在正向匹配和逆向匹配中对词进行划分\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义词的最大匹配长度。用于在正向匹配和逆向匹配中对词进行划分。\n",
    "max_matching = 10 \n",
    "\n",
    "# 初始化对象\n",
    "dict_seg = DictSegmentation(max_matching=max_matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合\n",
    "    def fit(self, file, split_lab,\n",
    "            save_model=\"DictSegmentation.pickle\",\n",
    "            del_start_str=None,\n",
    "            del_end_str=None,\n",
    "            regular_func=None,\n",
    "            ignore_punctuation=True):\n",
    "        \"\"\"\n",
    "        拟合词典\n",
    "        :param file: <str> 训练文件\n",
    "        :param split_lab: <str> 训练文本中对词的划分标记\n",
    "        :param save_model: <str> 保存模型的文件名\n",
    "        :param del_start_str: <str> 对于训练数据中的文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: <str> 对于训练数据中的文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :param ignore_punctuation: <bool> 是否忽略训练数据中的标点符号，即是否将标点符号加入到词典中\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86924it [00:00, 219482.90it/s]\n",
      "100%|██████████| 86924/86924 [40:07<00:00, 20.28it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save dictionary success! File: ../model/dict_segmentation.pickle\n",
      "word count： 88041\n"
     ]
    }
   ],
   "source": [
    "# 语料数据的训练文件\n",
    "train_file = '../data/icwb2-data/training/msr_training.utf8'\n",
    "save_model = '../model/dict_segmentation.pickle'\n",
    "\n",
    "dict_seg.fit(train_file, split_lab=' ', save_model=save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估\n",
    "    def eval(self, file, seg_lab=' ', w_file=\"test.txt\", encoding=\"utf-8\", threads=3, del_start_str=None,\n",
    "             del_end_str=None, regular_func=None):\n",
    "        \"\"\"\n",
    "        评估。采用线程并非机制，读取测试数据集，并进行分词\n",
    "        :param file: <str> 测试数据文本\n",
    "        :param seg_lab: <str> 分词标记。在分词完成之后，需要根据某个字符标记词汇，默认为 \" \"\n",
    "        :param w_file: <str> 分词结果写入的文件名\n",
    "        :param encoding: <str> 分词结果写入文件的编码格式\n",
    "        :param threads: <int> 启动线程数量\n",
    "        :param del_start_str: <str> 对于数据中的文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: <str> 对于数据中的文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3985it [00:00, 148943.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_0 start...\n",
      "thread_1 start...\n",
      "thread_2 start...\n",
      "thread_3 start...\n",
      "thread_4 start...\n",
      "thread_0 Process:797/797 \tthread_1 Process:797/797 \tthread_2 Process:797/797 \tthread_3 Process:797/797 \tthread_4 Process:797/797 \tTotal process: 3985/3985 Percentage:100.00%\n",
      "\n",
      "over！File: \u001b[48;0;31m../result/dict/msr_test_seg_result.utf8\u001b[0m, encoding: \u001b[48;0;31mutf-8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_file = '../data/icwb2-data/testing/msr_test.utf8'\n",
    "result = '../result/dict/msr_test_seg_result.utf8'\n",
    "\n",
    "dict_seg.eval(test_file, seg_lab=\"  \", w_file=result, threads=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评分\n",
    "\n",
    "使用下载数据集中提供的 scripts 评分脚本对测试数据集进行评分。详细查看 README .  下面摘自README 。\n",
    "\n",
    "* Scoring\n",
    "\n",
    "The script 'score' is used to generate compare two segmentations. The\n",
    "script takes three arguments:\n",
    "\n",
    "1. The training set word list\n",
    "2. The gold standard segmentation\n",
    "3. The segmented test file\n",
    "\n",
    "You must not mix character encodings when invoking the scoring\n",
    "script. For example:\n",
    "\n",
    "% perl scripts/score gold/cityu_training_words.utf8 gold/cityu_test_gold.utf8 test_segmentation.utf8 > score.ut8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl ../data/icwb2-data/scripts/score ../data/icwb2-data/gold/msr_training_words.utf8 ../data/icwb2-data/gold/msr_test_gold.utf8 ../result/dict/msr_test_seg_result.utf8 > ../result/dict/score.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看评分结果。评分结果输入到 ..／result/dict_seg_score.utf8 文件中. 结果在最后几行中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERTIONS:\t0\r\n",
      "DELETIONS:\t4\r\n",
      "SUBSTITUTIONS:\t4\r\n",
      "NCHANGE:\t8\r\n",
      "NTRUTH:\t45\r\n",
      "NTEST:\t41\r\n",
      "TRUE WORDS RECALL:\t0.822\r\n",
      "TEST WORDS PRECISION:\t0.902\r\n",
      "=== SUMMARY:\r\n",
      "=== TOTAL INSERTIONS:\t5095\r\n",
      "=== TOTAL DELETIONS:\t376\r\n",
      "=== TOTAL SUBSTITUTIONS:\t4194\r\n",
      "=== TOTAL NCHANGE:\t9665\r\n",
      "=== TOTAL TRUE WORD COUNT:\t106873\r\n",
      "=== TOTAL TEST WORD COUNT:\t111592\r\n",
      "=== TOTAL TRUE WORDS RECALL:\t0.957\r\n",
      "=== TOTAL TEST WORDS PRECISION:\t0.917\r\n",
      "=== F MEASURE:\t0.937\r\n",
      "=== OOV Rate:\t0.026\r\n",
      "=== OOV Recall Rate:\t0.025\r\n",
      "=== IV Recall Rate:\t0.983\r\n",
      "###\t../result/dict/msr_test_seg_result.utf8\t5095\t376\t4194\t9665\t106873\t111592\t0.957\t0.917\t0.937\t0.026\t0.025\t0.983\r\n"
     ]
    }
   ],
   "source": [
    "!tail -22 ../result/dict/score.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词\n",
    "\n",
    "    def cut(self, text, seg_lab=' ', del_start_str=None, del_end_str=None, regular_func=None):\n",
    "        \"\"\"\n",
    "        输入某个句子或文本，对其进行分词\n",
    "        :param text: <str> 句子文本\n",
    "        :param seg_lab: <str> 分词完成之后需要通过某个字符对其进行标记，默认为 \" \"\n",
    "        :param del_start_str: <str> 对于文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: <str> 对于文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :return: <str> 分词文本\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我/爱/吃/雪/糕/。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"我爱吃雪糕。\"\n",
    "dict_seg.cut(sentence, seg_lab=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加新词\n",
    "    def add_word(self, words, is_save=True, model_file='DictSegmentation.pickle'):\n",
    "        \"\"\"\n",
    "        添加词汇到训练词典中\n",
    "        :param words: <str, list, tuple> 词汇，可以是字符、列表、元祖\n",
    "        :param is_save: <bool> 对于新加的词汇，是否保存词典\n",
    "        :param model_file: <str> 需要保存的模型文件名\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save dictionary success! File: ../model/dict_segmentation.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我/爱/吃/雪糕/。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_seg.add_word([\"雪糕\"], is_save=True, model_file=save_model)\n",
    "dict_seg.cut(sentence, seg_lab=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型\n",
    "    def load(model_file, max_matching=MAX_MATCHING):\n",
    "        \"\"\"\n",
    "        加载词典分词对象\n",
    "        :param model_file: <str> 保存的词典文件，也叫模型文件\n",
    "        :param max_matching: <int> 最大匹配词长度，用于正向和逆向匹配的最大匹配词长度\n",
    "        :return: <DictSegmentation> 词典分词对象\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扬帆/远东/做/与/中国/合作/的/先行\n",
      "我/爱/吃/雪糕/。\n"
     ]
    }
   ],
   "source": [
    "d_seg = dict_seg.load(save_model, max_matching=max_matching)\n",
    "\n",
    "print(d_seg.cut(\"扬帆远东做与中国合作的先行\", seg_lab=\"/\"))\n",
    "print(d_seg.cut(\"我爱吃雪糕。\", seg_lab=\"/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram 分词\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram分词模型已封装在 module/segmentation/ngram/ngram_segmentation.py 类中。详细代码在可在此文件中查看。模型提供以下函数\n",
    "\n",
    " 1. 拟合：根据提供的训练数据创建词典。 并进行保存\n",
    " 2. 评估：读取测试数据，并对数据进行分词，写入文本\n",
    " 3. 分词：输入某个句子文本，对其进行分词\n",
    " 4. 加载模型：对拟合保存的模型进行加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from module.segmentation.ngram.ngram_segmentation import NgramSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "    def __init__(self, n=NGRAM, max_matching=MAX_MATCHING):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param n: <int> ngram 中的n元计算模式。n = {2,3}\n",
    "        :param max_matching: <int> 最大匹配长度默认=10\n",
    "        \"\"\"\n",
    " \n",
    " 备注：n = {2, 3} n = 3 的时候需要足够大的内存，否则容易出现内存错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "max_matching = 10\n",
    "\n",
    "ngram_seg = NgramSegmentation(n, max_matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合\n",
    "\n",
    "    def fit(self, file, split_lab, save_model=\"ngram_segmentation.pickle\", smoothing=None, del_start_str=None,\n",
    "            del_end_str=None, regular_func=None, is_save=False):\n",
    "        \"\"\"\n",
    "        拟合。输入训练数据集。创建词典、统计词频\n",
    "        :param file: <str> 文本数据\n",
    "        :param split_lab: <str> 训练数据集中的分词标记\n",
    "        :param save_model: <str> 保存模型\n",
    "        :param smoothing: <str> n 阶词频概率计算模式。目前只有 Laplace 模式\n",
    "        :param del_start_str: <str> 需要删除的开始字符\n",
    "        :param del_end_str: <str> 需要删除的结束字符\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :param is_save: <bool> 是否保存训练的词典。这里自测保存词典容易出现内存错误。等待优化处理\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86924it [00:00, 214035.73it/s]\n",
      "100%|██████████| 86924/86924 [00:02<00:00, 36357.77it/s]\n",
      "100%|██████████| 88183/88183 [00:00<00:00, 729859.09it/s]\n",
      "100%|██████████| 86918/86918 [00:05<00:00, 14791.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word number:  88184 total words number: 2542224\n"
     ]
    }
   ],
   "source": [
    "train_file = \"../data/icwb2-data/training/msr_training.utf8\"\n",
    "# save_model = \"../model/ngram_segmentation.pickle\"\n",
    "\n",
    "ngram_seg.fit(train_file, split_lab=\"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估\n",
    "\n",
    "    def eval(self, file, seg_lab=\"  \", w_file=\"test.txt\", encoding=\"utf-8\", threads=3, del_start_str=None,\n",
    "             del_end_str=None, regular_func=None):\n",
    "        \"\"\"\n",
    "        评估。输入测试数据。对测试数据进行分词。\n",
    "        :param file: <str> 测试数据文件名\n",
    "        :param seg_lab: <str> 分词标记。默认\"  \"， 即在分词完成之后，使用该标记区分\n",
    "        :param w_file: <str> 将结果写入的文件名\n",
    "        :param encoding: <str> 写入文件的编码格式，默认为utf-8\n",
    "        :param threads: <int> 线程数量\n",
    "        :param del_start_str: <str> 需要删除的开始字符\n",
    "        :param del_end_str: <str> 需要删除的结束字符\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3985it [00:00, 132908.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_0 start...\n",
      "thread_1 start...thread_2 start...\n",
      "\n",
      "thread_3 start...\n",
      "thread_4 start...\n",
      "thread_0 Process:797/797 \tthread_1 Process:797/797 \tthread_2 Process:797/797 \tthread_3 Process:797/797 \tthread_4 Process:797/797 \tTotal process: 3985/3985 Percentage:100.00%\n",
      "\n",
      "over！File: \u001b[48;0;31m../result/ngram/msr_test_seg_result.utf8\u001b[0m, encoding: \u001b[48;0;31mutf-8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_file = \"../data/icwb2-data/testing/msr_test.utf8\"\n",
    "result = \"../result/ngram/msr_test_seg_result.utf8\"\n",
    "\n",
    "ngram_seg.eval(test_file, seg_lab=\"  \", w_file=result, threads=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评分\n",
    "\n",
    "使用下载数据集中提供的 scripts 评分脚本对测试数据集进行评分。（同上）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl ../data/icwb2-data/scripts/score ../data/icwb2-data/gold/msr_training_words.utf8 ../data/icwb2-data/gold/msr_test_gold.utf8 ../result/ngram/msr_test_seg_result.utf8 > ../result/ngram/score.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERTIONS:\t0\r\n",
      "DELETIONS:\t4\r\n",
      "SUBSTITUTIONS:\t4\r\n",
      "NCHANGE:\t8\r\n",
      "NTRUTH:\t45\r\n",
      "NTEST:\t41\r\n",
      "TRUE WORDS RECALL:\t0.822\r\n",
      "TEST WORDS PRECISION:\t0.902\r\n",
      "=== SUMMARY:\r\n",
      "=== TOTAL INSERTIONS:\t4954\r\n",
      "=== TOTAL DELETIONS:\t389\r\n",
      "=== TOTAL SUBSTITUTIONS:\t3567\r\n",
      "=== TOTAL NCHANGE:\t8910\r\n",
      "=== TOTAL TRUE WORD COUNT:\t106873\r\n",
      "=== TOTAL TEST WORD COUNT:\t111438\r\n",
      "=== TOTAL TRUE WORDS RECALL:\t0.963\r\n",
      "=== TOTAL TEST WORDS PRECISION:\t0.924\r\n",
      "=== F MEASURE:\t0.943\r\n",
      "=== OOV Rate:\t0.026\r\n",
      "=== OOV Recall Rate:\t0.025\r\n",
      "=== IV Recall Rate:\t0.988\r\n",
      "###\t../result/ngram/msr_test_seg_result.utf8\t4954\t389\t3567\t8910\t106873\t111438\t0.963\t0.924\t0.943\t0.026\t0.025\t0.988\r\n"
     ]
    }
   ],
   "source": [
    "!tail -22 ../result/ngram/score.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词\n",
    "\n",
    "    def cut(self, text, seg_lab=' ', del_start_str=None, del_end_str=None, regular_func=None):\n",
    "        \"\"\"\n",
    "        分词。对某个文本或句子进行分词。\n",
    "        :param text: <str> 文本\n",
    "        :param seg_lab: <str> 分词标记。默认为 \" \" 。在分词完成之后，使用该标记标记完成的分词\n",
    "        :param del_start_str: <str> 需要删除的开始字符\n",
    "        :param del_end_str: <str> 需要删除的结束字符\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :return: <str> 分词结果\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'扬帆/远东/做/与/中国/合作/的/先行'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_seg.cut(\"扬帆远东做与中国合作的先行\", seg_lab=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM(隐马尔可夫) 分词\n",
    "\n",
    "hmm分词模型已封装在 module/segmentation/hmm/hmm_segmentation.py 类中。详细代码在可在此文件中查看。模型提供以下函数\n",
    "\n",
    "1. fit 拟合模型。输入训练数据集。创建发射概率矩阵和转移概率矩阵\n",
    "2. eval 评估。输入测试数据集。对测试数据进行分词，将分词结果写入文本\n",
    "3. cut 分词。输入文本，对文本进行分词\n",
    "4. load 加载模型。加载训练好的模型s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from module.segmentation.hmm.hmm_segmentation import HmmSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_seg = HmmSegmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合\n",
    "\n",
    "    def fit(self, file, split_lab, save_model=\"hmm_segmentation.pickle\", del_start_str=None, del_end_str=None,\n",
    "            regular_func=None):\n",
    "        \"\"\"\n",
    "        拟合\n",
    "        :param file: <str> 训练文件数据\n",
    "        :param split_lab: <str> 训练集中的分词标签\n",
    "        :param save_model: <str> 训练完成之后保存模型的文件名\n",
    "        :param del_start_str: <str>  对于文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: <str>  对于文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86924it [00:00, 209800.76it/s]\n",
      "100%|██████████| 86924/86924 [00:03<00:00, 24026.35it/s]\n",
      "100%|██████████| 86924/86924 [00:00<00:00, 90017.24it/s]\n",
      "100%|██████████| 5168/5168 [00:00<00:00, 854367.71it/s]\n",
      "86924it [00:04, 20313.92it/s]\n",
      "100%|██████████| 86924/86924 [00:04<00:00, 19643.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word number:  5168\n",
      "save dictionary success! File: ../model/hmm_segmentation.pickle\n"
     ]
    }
   ],
   "source": [
    "train_file = \"../data/icwb2-data/training/msr_training.utf8\"\n",
    "save_model = \"../model/hmm_segmentation.pickle\"\n",
    "\n",
    "hmm_seg.fit(train_file, split_lab=\"  \", save_model=save_model, del_start_str=\"“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估\n",
    "\n",
    "    def eval(self, file, seg_lab=\"  \", w_file=\"test.txt\", encoding=\"utf-8\", threads=3, del_start_str=None,\n",
    "             del_end_str=None, regular_func=None):\n",
    "        \"\"\"\n",
    "        评估。采用线程并非机制，读取测试数据集，并进行分词\n",
    "        :param file: <str> 测试数据文本\n",
    "        :param seg_lab: <str> 分词标记。在分词完成之后，需要根据某个字符标记词汇，默认为 \" \"\n",
    "        :param w_file: <str> 分词结果写入的文件名\n",
    "        :param encoding: <str> 分词结果写入文件的编码格式\n",
    "        :param threads: <int> 启动线程数量,默认为 3 \n",
    "        :param del_start_str: <str> 对于数据中的文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: <str> 对于数据中的文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3985it [00:00, 97043.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_0 start...thread_1 start...\n",
      "\n",
      "thread_2 start...\n",
      "thread_0 Process:1328/1328 \tthread_1 Process:1328/1328 \tthread_2 Process:1329/1329 \tTotal process: 3985/3985 Percentage:100.00%\n",
      "\n",
      "over！File: \u001b[48;0;31m../result/hmm/msr_test_seg_result.utf8\u001b[0m, encoding: \u001b[48;0;31mutf-8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_file = \"../data/icwb2-data/testing/msr_test.utf8\"\n",
    "result = \"../result/hmm/msr_test_seg_result.utf8\"\n",
    "\n",
    "import re\n",
    "def regular(sent):\n",
    "    # 删除文本中可能存在的空格字符\n",
    "    sent = re.sub(\"[ ]+\", '', sent)\n",
    "    return sent\n",
    "\n",
    "hmm_seg.eval(test_file, seg_lab=\"  \", w_file=result, regular_func=regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评分\n",
    "\n",
    "使用下载数据集中提供的 scripts 评分脚本对测试数据集进行评分。（同上）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl ../data/icwb2-data/scripts/score ../data/icwb2-data/gold/msr_training_words.utf8 ../data/icwb2-data/gold/msr_test_gold.utf8 ../result/hmm/msr_test_seg_result.utf8 > ../result/hmm/score.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERTIONS:\t2\r\n",
      "DELETIONS:\t3\r\n",
      "SUBSTITUTIONS:\t5\r\n",
      "NCHANGE:\t10\r\n",
      "NTRUTH:\t45\r\n",
      "NTEST:\t44\r\n",
      "TRUE WORDS RECALL:\t0.822\r\n",
      "TEST WORDS PRECISION:\t0.841\r\n",
      "=== SUMMARY:\r\n",
      "=== TOTAL INSERTIONS:\t7304\r\n",
      "=== TOTAL DELETIONS:\t5671\r\n",
      "=== TOTAL SUBSTITUTIONS:\t17396\r\n",
      "=== TOTAL NCHANGE:\t30371\r\n",
      "=== TOTAL TRUE WORD COUNT:\t106873\r\n",
      "=== TOTAL TEST WORD COUNT:\t108506\r\n",
      "=== TOTAL TRUE WORDS RECALL:\t0.784\r\n",
      "=== TOTAL TEST WORDS PRECISION:\t0.772\r\n",
      "=== F MEASURE:\t0.778\r\n",
      "=== OOV Rate:\t0.026\r\n",
      "=== OOV Recall Rate:\t0.363\r\n",
      "=== IV Recall Rate:\t0.796\r\n",
      "###\t../result/hmm/msr_test_seg_result.utf8\t7304\t5671\t17396\t30371\t106873\t108506\t0.784\t0.772\t0.778\t0.026\t0.363\t0.796\r\n"
     ]
    }
   ],
   "source": [
    "!tail -22 ../result/hmm/score.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词\n",
    "\n",
    "    def cut(self, text, seg_lab=' ', del_start_str=None, del_end_str=None, regular_func=None):\n",
    "        \"\"\"\n",
    "        分词。\n",
    "        :param text: <str> 文本。例如\"小明是中国人\"\n",
    "        :param seg_lab: <str> 分词标记，分词完成之后使用该标记，进行区分。默认问 ' '\n",
    "        :param del_start_str: <str>  对于文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: <str>  对于文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: <func> 正则化函数\n",
    "        :return: <str> 分词文本\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'扬帆/远东/做/与/中国/合作/的/先行'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_seg.cut(\"扬帆远东做与中国合作的先行\", seg_lab=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型\n",
    "\n",
    "    def load(model_file):\n",
    "        \"\"\"\n",
    "        加载模型\n",
    "        :param model_file: <str> 模型文件\n",
    "        :return: <HmmSegmentation> 分词模型\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'扬帆/远东/做/与/中国/合作/的/先行'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model = \"../model/hmm_segmentation.pickle\"\n",
    "\n",
    "h_mm = HmmSegmentation.load(save_model)\n",
    "h_mm.cut(\"扬帆远东做与中国合作的先行\", seg_lab=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF 分词\n",
    "\n",
    "  ● CRF 也叫条件随机场，解决了在 HMM 分词问题上不能做特征选择的问题。同时采用全局归一化，解决了最大熵隐马尔可夫模型出现的标注偏置的问题。\n",
    "  \n",
    "  ● CRF 优缺点：\n",
    "  \n",
    "      ○ 优点：文字词语出现的频率信息，同时考虑上下文语境，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果\n",
    "      ○ 缺点：训练周期较长，运营时计算量较大，性能不如词典分词\n",
    "  \n",
    "  ● 相关开源实现：\n",
    "  \n",
    "      ○ CRF++。  目前普遍认为比较好的分词工具包。但是目前没有可调用的API，只能根据提供的脚步使用。\n",
    "          ■ 相关中文分词参考：51nlp 比较详细的说明了CRF++分词的操作说明。\n",
    "          ■ CRF++ 文档\n",
    "      ○ Genius。https://github.com/duanhongyi/genius\n",
    "      ○ sklean_crfsuite.CRF: https://sklearn-crfsuite.readthedocs.io/en/latest/api.html\n",
    "\n",
    "模型使用 [sklearn_crfsuite](https://sklearn-crfsuite.readthedocs.io/en/latest/api.html) 进行构建。详细参考 module/segmentation/crf/crf_segmentation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from module.segmentation.crf.crf_segmentation import CRFSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "    def __init__(self, algorithm='lbfgs', min_freq=0, c1=0, c2=1.0, max_iterations=None):\n",
    "        \"\"\"\n",
    "        选用 sklearn_crfsuite AIP 文档中常用参数。详细的参数信息，可以参考 sklearn_crfsuite API 文档。\n",
    "        sklearn_crfsuite API 文档：https://sklearn-crfsuite.readthedocs.io/en/latest/api.html\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRFSegmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合\n",
    "\n",
    "    def fit(self, file, split_lab, save_model=\"crf_segmentation.pickle\", del_start_str=None, del_end_str=None,\n",
    "            regular_func=None):\n",
    "        \"\"\"\n",
    "        拟合模型\n",
    "        :param file: (str, mandatory) 训练数据\n",
    "        :param split_lab: (str, mandatory) 训练文本中对词的划分标记\n",
    "        :param save_model: (str, optional, default='crf_segmentation.pickle') 保存模型的文件名\n",
    "        :param del_start_str: (str, optional, default=None) 对于训练数据中的文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: (str, optional, default=None) 对于训练数据中的文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: (fun, optional, default=None)> 正则化函数\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86924it [00:00, 208464.88it/s]\n",
      "100%|██████████| 86924/86924 [00:07<00:00, 11977.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model over! File: ../model/crf_segmentation.pickle\n"
     ]
    }
   ],
   "source": [
    "train_file = \"../data/icwb2-data/training/msr_training.utf8\"\n",
    "save_model = \"../model/crf_segmentation.pickle\"\n",
    "\n",
    "crf.fit(train_file, split_lab=\" \", save_model=save_model, del_start_str=\"“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估\n",
    "\n",
    "    def eval(self, file, seg_lab=\"  \", w_file=\"test.txt\", encoding=\"utf-8\", threads=3, del_start_str=None,\n",
    "             del_end_str=None, regular_func=None):\n",
    "        \"\"\"\n",
    "        评估。\n",
    "        :param file: (str, mandatory) 测试数据\n",
    "        :param seg_lab: (str, mandatory, default=\"  \") 分词完成之后使用该标记区分\n",
    "        :param w_file: (str, optional, default=\"test.txt\") 将分词结果写入文件\n",
    "        :param encoding: (str, optional, default=\"utf-8\") 写入文件的编码格式\n",
    "        :param threads: (int optional, default=3) 执行线程数量\n",
    "        :param del_start_str: (str, optional, default=None) 对于训练数据中的文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: (str, optional, default=None) 对于训练数据中的文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: (fun, optional, default=None)> 正则化函数\n",
    "        :return:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3985it [00:00, 80155.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_0 start...\n",
      "thread_1 start...\n",
      "thread_2 start...\n",
      "thread_0 Process:1328/1328 \tthread_1 Process:1328/1328 \tthread_2 Process:1329/1329 \tTotal process: 3985/3985 Percentage:100.00%\n",
      "\n",
      "over！File: \u001b[48;0;31m../result/crf/msr_test_seg_result.utf8\u001b[0m, encoding: \u001b[48;0;31mutf-8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_file = \"../data/icwb2-data/testing/msr_test.utf8\"\n",
    "w_file = \"../result/crf/msr_test_seg_result.utf8\"\n",
    "\n",
    "import re\n",
    "def regular(sent):\n",
    "    # 删除文本中可能存在的空格字符\n",
    "    sent = re.sub(\"[ ]+\", '', sent)\n",
    "    return sent\n",
    "\n",
    "crf.eval(test_file, seg_lab=\"  \", w_file=w_file, regular_func=regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评分\n",
    "\n",
    "使用下载数据集中提供的 scripts 评分脚本对测试数据集进行评分。（同上）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl ../data/icwb2-data/scripts/score ../data/icwb2-data/gold/msr_training_words.utf8 ../data/icwb2-data/gold/msr_test_gold.utf8 ../result/crf/msr_test_seg_result.utf8 > ../result/crf/score.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERTIONS:\t2\r\n",
      "DELETIONS:\t1\r\n",
      "SUBSTITUTIONS:\t8\r\n",
      "NCHANGE:\t11\r\n",
      "NTRUTH:\t45\r\n",
      "NTEST:\t46\r\n",
      "TRUE WORDS RECALL:\t0.800\r\n",
      "TEST WORDS PRECISION:\t0.783\r\n",
      "=== SUMMARY:\r\n",
      "=== TOTAL INSERTIONS:\t4506\r\n",
      "=== TOTAL DELETIONS:\t4801\r\n",
      "=== TOTAL SUBSTITUTIONS:\t11275\r\n",
      "=== TOTAL NCHANGE:\t20582\r\n",
      "=== TOTAL TRUE WORD COUNT:\t106873\r\n",
      "=== TOTAL TEST WORD COUNT:\t106578\r\n",
      "=== TOTAL TRUE WORDS RECALL:\t0.850\r\n",
      "=== TOTAL TEST WORDS PRECISION:\t0.852\r\n",
      "=== F MEASURE:\t0.851\r\n",
      "=== OOV Rate:\t0.026\r\n",
      "=== OOV Recall Rate:\t0.587\r\n",
      "=== IV Recall Rate:\t0.857\r\n",
      "###\t../result/crf/msr_test_seg_result.utf8\t4506\t4801\t11275\t20582\t106873\t106578\t0.850\t0.852\t0.851\t0.026\t0.587\t0.857\r\n"
     ]
    }
   ],
   "source": [
    "!tail -22 ../result/crf/score.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词\n",
    "\n",
    "    def cut(self, text, seg_lab=' ', del_start_str=None, del_end_str=None, regular_func=None):\n",
    "        \"\"\"\n",
    "        分词。\n",
    "        :param text: (str, mandatory) 字符文本或句子\n",
    "        :param seg_lab: (str, optional, default=\"  \") 分词完成之后使用该标记区分\n",
    "        :param del_start_str: (str, optional, default=None) 对于训练数据中的文本句子，是否存在开始标记需要删除，如果有，则输入\n",
    "        :param del_end_str: (str, optional, default=None) 对于训练数据中的文本句子，是否存在结束标记需要删除，如果有，则输入\n",
    "        :param regular_func: (fun, optional, default=None)> 正则化函数\n",
    "        :return: (str) 分词句子\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'种田/要/有/个明/白账/，/投本/要/赚/利润/是/起码/的/道理/。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"种田要有个明白账，投本要赚利润是起码的道理。\"\n",
    "\n",
    "crf.cut(sent, seg_lab=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型\n",
    "\n",
    "    def load(file):\n",
    "        \"\"\"\n",
    "        加载模型。\n",
    "        :param file: (str, mandatory) 模型文件\n",
    "        :return: (MFSSegmentation) 分词对象\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'种田/要/有/个明/白账/，/投本/要/赚/利润/是/起码/的/道理/。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"../model/crf_segmentation.pickle\"\n",
    "\n",
    "crf_model = CRFSegmentation.load(model)\n",
    "crf_model.cut(sent, seg_lab=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
